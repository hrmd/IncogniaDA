---
title: "Incognia Data Analysis"
author: "Hugo Daher"
date: "`r Sys.Date()`"
output_dir: "."
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![](Dados/Incognia-Logo-Purple-735x129.png)

# Incognia Data Analysis Report

## Introduction
Statistical analysis is a efficient way to get insights about any data, 
making anyone able to ask the right question. In addition, using the 
best technologies is crucial to get better results and faster. 
Therefore, this analysis used the powerful Language R to load and 
transform the dataset provided, creating significant visualizations 
with RStudio tool. Thus, this report has the result of statistical 
thinking that collected meaningful data to answer the right question 
with detailed analysis of patterns found in the data, making possible 
to draw conclusions that go beyond the observed data.

## Dataset description
Each event in the dataset analysed was a login to the client's app and 
the purpose of the analysis was to find patterns related to accounts and 
devices that indicate suspicious behavior, witch is possibly associated 
with fraud. The dataset contains records from July 2021 and it's a great 
dataset for evaluating linear regression models.

## Initial dataset schema
* event_id: identifier of the event.
* event_timestamp: event datetime in milliseconds.
* account_id: identifier of the account associated with the event.
* device: identifier of the device that performed the operation.
* distance_to_frequent_location: distance (in meters) from the device, 
at the time of the event, to one of the frequent locations related to 
the account.
* device_age_days_days: days since an account appeared related to a device.
* is_emulator: indicates whether the device is an emulator.
* has_has_fake_location: indicates whether the device was using false 
locations at the time of the operation.
* has_has_root_permissions: indicates whether the device has device 
administrator permissions.
* app_is_from_app_is_from_official_store: indicates whether or not the app used 
to perform the operation came from an official store.


***

## Dataset processing

```{r Libraries, echo=FALSE, eval=FALSE}
#knitr::opts_chunk$set(warning = FALSE, message = FALSE)
### Loading libraries
library(dplyr)
library(data.table)
library(ggplot2)
library(Amelia)
library(caret)
library(reshape)
library(randomForest)
library(e1071)
library(tidyr)
library(DMwR)
library(GGally)
library(stringr)
library(gridExtra)
library(scales)
library(viridis)
library(hrbrthemes)
library(psych)

```

### Loading dataset

```{r Dataset}

tb <- read.table("Dados/hugo_incognia_db_for_da_test.csv",
                    dec = ".",
                    sep = ",",
                    h = T,
                    fileEncoding = "windows-1252")
```

### Dataset general information

Dataset size is *`r dim(tb)[1]`* rows and *`r dim(tb)[2]`* columns


> Compactly displaying the structure of dataset

```{r 2-STR, echo=FALSE}
str(tb)
```

### Dataset transformation

> Removing id column, not necessary for analysis

```{r 3-RevomeID, eval=FALSE}
tb$event_id <- NULL
```

> Shortening some column names

```{r 4-Names, eval=FALSE, include=FALSE}
names(tb) <- c("event_timestamp","account_id","device",
               "distance_to_frequent_location","device_age_days","is_emulator",
               "has_fake_location","has_root_permissions","app_is_from_official_store")
```

> Converting variable to integer

```{r 5-Integer, eval=FALSE}
tb$distance_to_frequent_location <- 
  as.integer(tb$distance_to_frequent_location)
```

> Converting miliseconds to timestamp

```{r 6-Milliseconds}
tb$event_timestamp <- as.POSIXct(as.integer(tb$event_timestamp) / 1000, 
                                 origin = "1970-01-01", 
                                 tz = "UTC")
summary(tb$event_timestamp)
```

> Creating variable Event Hour, Date and Weekday

```{r 7-Dates}
tb$event_hour <- format(tb$event_timestamp,"%H")
tb$event_hour <- as.numeric(tb$event_hour)
tb$event_date <- as.Date(tb$event_timestamp,format="%Y-%m-%d")

#Sys.setlocale("LC_TIME","Portuguese")
#Sys.getlocale("LC_TIME")
tb$weekday <- weekdays(tb$event_date)
tb$wday <- as.integer(as.POSIXlt(tb$event_date)$wday)
```

> Missing values

```{r 8-Missing, eval=FALSE, include=FALSE}
sapply(tb, function(x) sum(is.na(x)))
Amelia::missmap(tb, main = "Missing Values")
```

There are `r sum(is.na(tb))` missing values. However, all missing values are in
a location variable, which is crucial to determine risk because it's a login
attempt without location information. Solution was replace for -1 instead
removing.

```{r 9-Replacing_NA}
tb <- tidyr::replace_na(tb, list(distance_to_frequent_location = -1))
```

> Creating a Device Age Category

```{r 10-AgeCat}
tb$device_age_category <- cut(tb$device_age_days, 
            breaks = c(0,1,7,30,365,Inf),
            labels = c("Day","Week","Month","Year","Year+"), right = FALSE)
```

> Creating a Distance to frequent location Category

```{r 11-DistGroup}
tb$dtfl_category <- cut(tb$distance_to_frequent_location, 
                              breaks = c(-1,0,1,10,Inf),
                              labels = c("No Location",
                                         "Freq Location",
                                         "Near FL",
                                         "Far FL"), right = FALSE)
```

> Creating Risk Score variable

```{r 12-RiskScore}
tb$score_risk = 0
tb$score_risk <- tb$score_risk + 
  ifelse(tb$event_hour>=0 & tb$event_hour<=6,1,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$dtfl_category=="No Location",3,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$dtfl_category=="Far FL",2,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$dtfl_category=="Near FL",1,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$device_age_category=="Day",3,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$is_emulator=="true",3,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$has_fake_location=="true",3,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$has_root_permissions=="true",3,0)
tb$score_risk <- tb$score_risk + 
  ifelse(tb$app_is_from_official_store=="false",3,0)
```

> Creating Risk Level (category based on score)

```{r 13-RiskLevel}
tb$risk_level <- cut(tb$score_risk, 
                              breaks = c(0,2,3,Inf),
                              labels = c("Low","Medium","High"), right = FALSE)
```

> Creating a single column with all numeric variables
> and building a numeric only dataframe

```{r 14-tb_pivot}
library(tidyr)
library(dplyr)
tb_pivot <- pivot_longer(select
                         (tb,c("distance_to_frequent_location",
                               "device_age_days",
                               "event_hour",
                               "score_risk")),
                             cols = everything(), names_to = "numeric_var",
                             values_to = "values")

tb_num <- select(tb,c("distance_to_frequent_location",
                      "device_age_days",
                      "event_hour",
                      "score_risk"))
```

> Absolute and relative frequency

```{r 15-Freq}
# Absolute frequency
freq1 <- table(tb[c('is_emulator')])
freq2 <- table(tb[c('has_fake_location')])
freq3 <- table(tb[c('has_root_permissions')])
freq4 <- table(tb[c('app_is_from_official_store')])
freq5 <- table(tb[c('dtfl_category')])
freq6 <- table(tb[c('device_age_category')])
freq7 <- table(tb[c('event_hour')])
freq8 <- table(tb[c('risk_level')])

# Relative frequency
freq_rel1 <- round(prop.table(freq1),7)*100
freq_rel2 <- round(prop.table(freq2),7)*100
freq_rel3 <- round(prop.table(freq3),7)*100
freq_rel4 <- round(prop.table(freq4),7)*100
freq_rel5 <- round(prop.table(freq5),4)*100
freq_rel6 <- round(prop.table(freq6),4)*100
freq_rel7 <- round(prop.table(freq7),4)*100
freq_rel8 <- round(prop.table(freq8),4)*100
```

### Variables added to the dataset schema
* event_hour: hour of the event day.
* event_date: event date (yyyy-mm-dd)
* device_age_category: variable device_age_days sliced in a category 
(**"Day"**,**"Week"**,**"Month"**,**"Year"**,**"Year+"**)
* dtfl_category: variable distance_to_frequent_location sliced in a 
category (**"No Location"**,**"Freq Location"**,**"Near FL"**,**"Far FL"**)
* score_risk: variable that accumulates points whenever another 
variable indicates some risk
* risk_level: variable score_risk sliced in a category 
(**"Low"**,**"Medium"**,**"High"**)


### Graphical representation of information and data 

> Evaluating correlation between variables

```{r 16-ggpairs, eval=FALSE, include=FALSE}
GGally::ggpairs(tb_num, upper = 
                  list(continuous = 
                         GGally::wrap("cor", size = 3))) +
  ggplot2::theme(
    axis.text = ggplot2::element_text(size = 6),
    axis.title = ggplot2::element_text(size = 6),
    legend.background = ggplot2::element_rect(fill = "white"),
    panel.grid.major = ggplot2::element_line(colour = NA),
    panel.grid.minor = ggplot2::element_blank(),
    panel.background = ggplot2::element_rect(fill = "grey95"))
```

> Heatmap analysis

```{r 17-Heatmap}
M = cor(tb_num)
corrplot::corrplot(M, method = 'color', order = 'alphabet')
```

> Barplot analysis

```{r 18-Barplot, echo=FALSE}
library(ggplot2)
library(scales)
# Risk Level
df = data.frame(freq8)
bar1 <- ggplot(data=df, aes(x=Var1, y=Freq, fill=Var1)) +
  geom_bar(stat="identity") +
  scale_fill_manual("legend",values=c("turquoise","khaki",2)) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Risk Level") +
  labs(y = "Count", 
       x = "Risk Level") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Is Emulator
#temp <- summarise(group_by(tb,is_emulator),qtde = length(is_emulator))
#df = data.frame(temp)
df = data.frame(freq1)
bar2 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill=c(2,3)) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Is Emulator") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Has Fake Location
df = data.frame(freq2)
bar3 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill=c(2,3)) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Has Fake Location") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Has Root Permissions
df = data.frame(freq3)
bar4 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill=c(2,3)) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Has Root Permissions") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# App is from official store
df = data.frame(freq4)
bar5 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill=c(2,"turquoise")) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("App from official store") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Distance to FL Category
df = data.frame(freq5)
bar6 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity",fill=c(2,"turquoise4","turquoise","khaki")) +
#  scale_fill_manual("legend",values=c("turquoise","khaki",2)) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Distance to Frequent Location Category") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Device Age Category
df = data.frame(freq6)
bar7 <- ggplot(data=df, aes(x=Var1, y=Freq)) +
  geom_bar(stat="identity", fill=c(2,"thistle","khaki","turquoise","turquoise4")) +
  geom_text(aes(label=Freq), vjust=0, size=3)+
  ggtitle("Device Age Category") +
  labs(y = "Count", 
       x = "Category") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

```{r, echo=FALSE }
# Hour analysis
df = data.frame(freq7)
bar8 <- ggplot(data=df, aes(y=Freq)) +
  geom_bar(stat="identity", fill=c(2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3)) +
#  geom_text(aes(label=Freq), vjust=-0.3, size=3.5)+
  ggtitle("Hour of the Day") +
  labs(y = "Count", 
       x = "Hour") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)

```

>Barplot Grid

```{r 19b-Grids}
library(gridExtra)

grid.arrange(bar1, bar6, nrow=2, ncol=1)

grid.arrange(bar7, bar8, nrow=2, ncol=1)

grid.arrange(bar2, bar3, bar4, bar5, nrow=2, ncol=2)
```

> Pie Chart

```{r 20-Pies, eval=FALSE, include=FALSE}
# set the plotting area into a 1*3 array
par(mfrow=c(1,2))    

pc <- pie(freq8, labels = freq8 , 
          main = "Risk Level",
          col = c("turquoise","khaki",2))
legend("topright", names(freq8), cex = .8,
       fill = c("turquoise","khaki",2))

pc <- pie(freq_rel6, labels = freq_rel6 , 
          main = "Device Age Analysis",
          col = c(2,"thistle","khaki","turquoise","turquoise4"))
legend("topright", names(freq_rel6), cex = .7,
       fill = c(2,"thistle","khaki","turquoise","turquoise4"))

par(mfrow=c(1,1))
pc <- pie(freq_rel5, labels = freq_rel5 , 
          main = "Distance to Frequent Location",
          col = c(2,"turquoise4","turquoise","khaki"))
legend("topright", names(freq_rel5), cex = .8,
       fill = c(2,"turquoise4","turquoise","khaki"),)
```

> Boxplot analysis

```{r 21-Boxplot, echo=FALSE}
# Numeric variables
ggplot(tb_pivot, mapping = aes(x=numeric_var, y=values, fill="red")) +
  geom_boxplot(show.legend = FALSE) + 
  facet_wrap(.~numeric_var, ncol = 4, scales = "free") +
  ggtitle("Numeric Variables Analysis") +
  labs(x = "Numeric Variables") +
  theme(strip.text.x = element_blank(), 
        text = element_text(size = 12, color = "turquoise4")) +
  scale_y_continuous(labels = comma)

# Is Emulator vs Device Age
box1 <- ggplot(tb, aes(x = reorder(is_emulator, device_age_days), 
                       y = device_age_days, fill = is_emulator)) + 
  geom_boxplot() +
#  ggtitle("Is Emulator vs Device Age") +
  labs(y = "Device Age Days", 
       x = "Is Emulator") +
  theme(legend.position = "none", 
        title = element_text(size = 12, color = "turquoise4"))

# Has Fake Location vs Device Age
box2 <- ggplot(tb, aes(x = reorder(has_fake_location, device_age_days), 
                       y = device_age_days, fill = has_fake_location)) + 
  geom_boxplot() +
#  ggtitle("Has Fake Location vs Device Age") +
  labs(y = "Device Age Days", 
       x = "Has Fake Location") +
  theme(legend.position = "none", 
        title = element_text(size = 12, color = "turquoise4"))

# Has Root Permissions vs Device Age
box3 <- ggplot(tb, aes(x = reorder(has_root_permissions, device_age_days), 
                       y = device_age_days, fill = has_root_permissions)) + 
  geom_boxplot() +
#  ggtitle("Has Root Permissions vs Device Age") +
  labs(y = "Device Age Days", 
       x = "Has Root Permissions") +
  theme(legend.position = "none", 
        title = element_text(size = 12, color = "turquoise4"))

# App From Official Store vs Device Age
box4 <- ggplot(tb, 
               aes(x = reorder(app_is_from_official_store, device_age_days), 
                   y = device_age_days, fill = app_is_from_official_store)) + 
  geom_boxplot() +
#  ggtitle("App From Official Store vs Device Age") +
  labs(y = "Device Age Days", 
       x = "App From Official Store") +
  theme(legend.position = "none", 
        title = element_text(size = 12, color = "turquoise4"))
```

>Boxplot Grid

```{r 21b-Grid}
library(gridExtra)
# GRid
grid.arrange(box1, box2, box3, box4, nrow=2, ncol=2)

# Distance to Frequent Location Category vs Device Age
ggplot(tb, aes(x = dtfl_category, 
               y = device_age_days, fill = dtfl_category)) + 
  geom_boxplot() +
  ggtitle("Distance to Frequent Location Category vs Device Age") +
  labs(y = "Device Age Days", 
       x = "Distance to Frequent Location category") +
  theme(legend.position = "none", 
        title = element_text(size = 12, color = "turquoise4"))
```

> Scatter Plot analysis 

```{r 22-Scatter, eval=FALSE, include=FALSE}
# showing relation between two variables and a logical variable
ggplot(data = tb, aes(y = distance_to_frequent_location/1000, 
                      x = device_age_days)) +
  geom_point(aes(color = risk_level), size = 2) +
  ggtitle("Distance to frequent location vs Device Age") +
  scale_color_manual(values=c("turquoise4","gold",2)) +
  labs(y = "Distance to frequent location (Km)", 
       x = "Device Age (days)") +
  theme(title = element_text(size = 12, color = "turquoise4"))
```

> Histogram of distribution

```{r 23-Histogram}
# Device Age
#
temp <- summarise(group_by(tb,device),xmax = max(device_age_days))
df = data.frame(value = temp$xmax)
ggplot(df, aes(x=value)) + 
  geom_histogram(col="white",aes(fill = ..count..)) +
  ggtitle("Histogram with distribution curve of Device Age") +
  labs(y = "Count", 
       x = "Device Age (days)") +
  theme(title = element_text(size = 12, color = "turquoise4"))
```

> Date analysis

```{r 24-DateCharts, eval=FALSE, include=FALSE}
# Event Date
temp <- dplyr::summarise(dplyr::group_by(tb,risk_level,event_date),qtde = length(event_date))
df = data.frame(temp)
ggplot(df, aes(x=event_date,y=qtde, fill=risk_level)) + 
  geom_area(alpha=0.6 , size=1.0, colour="#ffffff") +
  scale_fill_manual(values=c("turquoise","khaki",2)) +
  ggtitle("Distribution curve of Event Date") +
  labs(y = "Count", 
       x = "Event Date") +
  theme(legend.position='right',
        title = element_text(size = 12, color = "turquoise4"))

# Event Weekday
temp <- summarise(group_by(tb,risk_level,weekday,wday),qtde = length(weekday))
df = data.frame(temp)
ggplot(df, aes(x=reorder(weekday,wday),
               y=qtde, fill=risk_level)) + 
  geom_bar(stat="identity", position="stack") +
  scale_fill_manual(values=c("turquoise","khaki",2)) +
  ggtitle("Day of the Week") +
  labs(y = "Count", 
       x = "Weekday") +  
  theme(title = element_text(size = 10, color = "turquoise4")) +
  scale_y_continuous(labels = comma)
```

### Analysis Coclusion
The dataset analysis found patterns related to the accounts and devices that 
indicate suspicious behavior, possibly associated with fraud. In this report, 
it is possible to understand how these patterns occur and how often. Moreover, 
with the patterns found, it was possible to create a variable that measures 
risk with a score points. The higher the score, the greater the risk. 
In addition, another variable was defined to categorize the risk level into 
**Low**, **Medium** and **High**. Therefore, Incognia will be able to improve the 
detection algorithm and with that, increase the efficiency in the 
communication with the financial client in order to avoid fraud. 

***

## Machine Learning Model

### Objective
Create a machine learning model to predict whether a financial 
event is a fraud event based on patterns found in a dataset.

### Preparing data to the model

> Sample random rows in dataframe

Only 3000 rows were selected at random from the dataset. 
More than this value requires a higher computational level.
```{r}
library(dplyr)
library(data.table)
library(ggplot2)
library(Amelia)
library(caret)
library(reshape)
library(randomForest)
library(e1071)
library(tidyr)
library(DMwR)
library(GGally)
library(stringr)
library(gridExtra)
library(scales)
library(viridis)
library(hrbrthemes)
library(psych)
# Sample random rows in dataframe
df = data.frame(tb)
tb_ml <- df[sample(nrow(df), 3000), ]

# Suggesting a variable as a possible fraud 
# Risk Score greater than 3
tb_ml$is_fraud <- ifelse(tb_ml$score_risk>=4,1,0)

tb_ml$is_fraud <- as.factor(tb_ml$is_fraud)
tb_ml$account_id <- NULL #as.factor(tb_ml$account_id)
tb_ml$device <- NULL #as.factor(tb_ml$device)
tb_ml$event_hour <- as.factor(tb_ml$event_hour)
tb_ml$score_risk <- as.factor(tb_ml$score_risk)
tb_ml$event_timestamp <- NULL #as.factor(tb_ml$event_timestamp)
tb_ml$is_emulator <- as.factor(tb_ml$is_emulator)
tb_ml$has_fake_location <- as.factor(tb_ml$has_fake_location)
tb_ml$has_root_permissions <- as.factor(tb_ml$has_root_permissions)
tb_ml$app_is_from_official_store <- as.factor(tb_ml$app_is_from_official_store)
tb_ml$event_date <- NULL #as.factor(tb_ml$event_date)
#tb_ml$distance_to_frequent_location <- as.integer(tb_ml$distance_to_frequent_location)
#tb_ml$device_age_days <- as.integer(tb_ml$device_age_days)
tb_ml$weekday <- as.factor(tb_ml$weekday)
tb_ml$wday <- NULL

str(tb_ml)

# Set seed
set.seed(12345)

# Selecting rows according to variable IS_FRAUD
index <- createDataPartition(tb_ml$is_fraud, p = 0.75, list = FALSE)
dim(index)

# Setting training data as a subset
data_training <- tb_ml[index,]
dim(data_training)
table(data_training$is_fraud)

# Percentages between classes
prop.table(table(data_training$is_fraud))

# Percentage comparison between training classes and original dataset
data_comparison <- cbind(prop.table(table(data_training$is_fraud)), 
                       prop.table(table(tb_ml$is_fraud)))
colnames(data_comparison) <- c("Training", "Original")
data_comparison

# Melt Data - Convert columns to rows
melt_data_comparison <- melt(data_comparison)
melt_data_comparison

# Plot - Training vs original
ggplot(melt_data_comparison, aes(x = X1, y = value)) + 
  geom_bar( aes(fill = X2), stat = "identity", position = "dodge") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Everything not in the Training Dataset must be in the Test Dataset
data_test <- tb_ml[-index,]
dim(data_test)
dim(data_training)

# Building model first version
View(data_training)
model_v1 <- randomForest(is_fraud ~ ., data = data_training)
model_v1
plot(model_v1)

# Test data predicts
predict_v1 <- predict(model_v1, data_test)

# Confusion Matrix to calculate a cross-tabulation of observed and predicted classes
cm_v1 <- caret::confusionMatrix(predict_v1, data_test$is_fraud, positive = "1")
cm_v1

# Precision, Recall e F1-Score, measures to evaluate predict model
y <- data_test$is_fraud
y_pred_v1 <- predict_v1

precision <- posPredValue(y_pred_v1, y)
precision

recall <- sensitivity(y_pred_v1, y)
recall

F1 <- (2 * precision * recall) / (precision + recall)
F1

# SMOTE algorithm for unbalanced classification problems

table(data_training$is_fraud)
prop.table(table(data_training$is_fraud))
set.seed(9560)
#str(data_test)
#temp_data_training <- data_training 
#temp_data_training$app_is_from_official_store <- NULL

data_training_bal <- data_training #SMOTE(is_fraud ~ ., data  = data_training)                         
table(data_training_bal$is_fraud)
prop.table(table(data_training_bal$is_fraud))

# Building model version 2
model_v2 <- randomForest(is_fraud ~ ., data = data_training_bal)
model_v2
plot(model_v2)

# Test data predictions
predict_v2 <- predict(model_v2, data_test)

# Confusion Matrix
cm_v2 <- caret::confusionMatrix(predict_v2, data_test$is_fraud, positive = "1")
cm_v2

# Precision, Recall e F1-Score
y <- data_test$is_fraud
y_pred_v2 <- predict_v2

precision <- posPredValue(y_pred_v2, y)
precision

recall <- sensitivity(y_pred_v2, y)
recall

F1 <- (2 * precision * recall) / (precision + recall)
F1

# Most important variables to predict
varImpPlot(model_v2)
imp_var <- importance(model_v2)
varImportance <- data.frame(Variables = row.names(imp_var), 
                            Importance = round(imp_var[ ,'MeanDecreaseGini'],2))
rankImportance <- varImportance %>% 
  mutate(Rank = paste0('#', dense_rank(desc(Importance))))

# Visualizing related importance
ggplot(rankImportance, 
       aes(x = reorder(Variables, Importance), 
           y = Importance, 
           fill = Importance)) + 
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank), 
            hjust = 0, 
            vjust = 0.55, 
            size = 4, 
            colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() 

# Building model version 3 with the most important variables
colnames(data_training_bal)
model_v3 <- randomForest(is_fraud ~ device_age_days + device_age_category + score_risk + risk_level + event_hour + distance_to_frequent_location, 
                          data = data_training_bal)
model_v3
plot(model_v3)

# Test data predictions
predict_v3 <- predict(model_v3, data_test)

# Confusion Matrix
cm_v3 <- caret::confusionMatrix(predict_v3, data_test$is_fraud, positive = "1")
cm_v3

# Precision, Recall e F1-Score
y <- data_test$is_fraud
y_pred_v3 <- predict_v3

precision <- posPredValue(y_pred_v3, y)
precision

recall <- sensitivity(y_pred_v3, y)
recall

F1 <- (2 * precision * recall) / (precision + recall)
F1

# Salve model file
#saveRDS(model_v3, file = "Dados/model_v3.rds")

# Loading model
#final_model <- readRDS("Dados/model_v3.rds")
final_model <- model_v3

# Predictions with new data from 3 events

# Events data
device_age_days <- c(0, 10, 100) 
device_age_category <- c("Day", "Month", "Year") 
score_risk <- c("3", "0", "0") 
risk_level <- c("High", "Low", "Low") 
event_hour <- c("4", "10", "11") 
distance_to_frequent_location <- c(1000.0, 0.001, 0.001) 

new_events <- data.frame(device_age_days, device_age_category, score_risk, risk_level, event_hour, distance_to_frequent_location)

new_events$device_age_category <- factor(new_events$device_age_category, levels = levels(data_training_bal$device_age_category))
new_events$score_risk <- factor(new_events$score_risk, levels = levels(data_training_bal$score_risk))
new_events$risk_level <- factor(new_events$risk_level, levels = levels(data_training_bal$risk_level))
new_events$event_hour <- factor(new_events$event_hour, levels = levels(data_training_bal$event_hour))

# Predictions
pred_new_events <- predict(final_model, new_events)
df = data.frame(pred_new_events)
df$is_fraud <- ifelse(pred_new_events==0,"No","Yes")
View(df)

```

### Model Results
To create a machine learning model it would be important to have a sample 
with descriptive information determining whether each event is a fraud 
event. With this information, it would be possible to create a 
prediction algorithm model to predict which future events will 
be a fraud event based on the training and testing model.

As the dataset did not contain the information (yes/no fraud), then a 
risk level was suggested based on the patterns found. An experiment 
was done based on the hypothesis that a score of 4+ is a fraud event. 
However, the rules that defined this score make the confidence 
level of the experiment result in 100%. That is, the model does 
not make mistakes in the predictions. Therefore, the ideal is to 
have the information (yes/no fraud) to create a predictive model 
without being biased.

